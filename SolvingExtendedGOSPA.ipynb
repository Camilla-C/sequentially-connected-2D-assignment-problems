{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc90571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_data(data):\n",
    "    \n",
    "    dataX = scipy.io.loadmat('X_' + data + '.mat')\n",
    "    dataY = scipy.io.loadmat('Y_' + data + '.mat')\n",
    "    X = dataX['temp']\n",
    "    Y = dataY['temp']\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a69b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_D(X,Y,p,c):\n",
    "    \n",
    "    n_X = len(X[0,0])\n",
    "    n_Y = len(Y[0,0])\n",
    "    T = len(X[0])\n",
    "    D = np.zeros((T,n_X+1,n_Y+1))\n",
    "    cp = c ** p\n",
    "    cp2 = cp/2\n",
    "    \n",
    "    for t in range(T):\n",
    "        for i in range(n_X+1):\n",
    "            X_exists = True\n",
    "            if i == n_X:\n",
    "                X_exists = False\n",
    "            elif any(np.isnan(X[:,t,i])):\n",
    "                X_exists = False\n",
    "            \n",
    "            for j in range(n_Y+1):\n",
    "                Y_exists = True\n",
    "                if j == n_Y:\n",
    "                    Y_exists = False\n",
    "                elif any(np.isnan(Y[:,t,j])):\n",
    "                    Y_exists = False\n",
    "                \n",
    "                if X_exists and Y_exists:\n",
    "                    D[t,i,j] = min(cp, (np.linalg.norm(X[:,t,i] - Y[:,t,j], ord=p)) ** p)\n",
    "                    \n",
    "                elif not X_exists and not Y_exists:\n",
    "                    pass\n",
    "                    \n",
    "                else:\n",
    "                    D[t,i,j] = cp2\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406e05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Update_W(D, beta):\n",
    "    \n",
    "    n_X = len(D) - 1\n",
    "    n_Y = len(D[0]) - 1\n",
    "    \n",
    "    temp1 = D[0:n_X,0:n_Y] + beta\n",
    "    temp2 = np.transpose(np.tile(D[0:n_X,n_Y], (n_X,1)))\n",
    "    temp3 = np.tile(D[n_X,0:n_Y], (n_Y,1))\n",
    "    temp4 = np.zeros((n_Y,n_X))\n",
    "\n",
    "    cost = np.block([[temp1, temp2],[temp3, temp4]])\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(cost)\n",
    "    \n",
    "    row_ind = [n_X if x>n_X else x for x in row_ind]\n",
    "    col_ind = [n_Y if x>n_Y else x for x in col_ind]\n",
    "    \n",
    "    W = np.zeros((n_X + 1, n_Y + 1))\n",
    "    W[row_ind,col_ind] = 1\n",
    "    W[n_X,n_Y] = 0\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e22a376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_upper_bound(D, switching_penalty):\n",
    "    \n",
    "    ############ Get input and def constants ###################################################\n",
    "    n_X = len(D[0]) - 1\n",
    "    n_Y = len(D[0,0]) - 1\n",
    "    T = len(D)\n",
    "    \n",
    "    ############ Initilization #################################################################\n",
    "    W = np.zeros((T, n_X+1, n_Y+1))\n",
    "\n",
    "    for iterations in range(T):\n",
    "        \n",
    "        ############ update W ##################################################################\n",
    "        W_prev = np.copy(W)\n",
    "        beta = switching_penalty * (np.ones((n_X, n_Y)) - W_prev[1,0:n_X,0:n_Y] -10**-3) \n",
    "        W[0,:,:] = Update_W(D[0,:,:], beta)\n",
    "        \n",
    "        for t in range(1,T-1):\n",
    "            beta = switching_penalty * (np.ones((n_X, n_Y)) - W_prev[t-1,0:n_X,0:n_Y] - W_prev[t+1,0:n_X,0:n_Y] -10**-3)\n",
    "            W[t,:,:] = Update_W(D[t,:,:], beta)\n",
    "           \n",
    "        beta = switching_penalty * (np.ones((n_X, n_Y)) - W_prev[T-2,0:n_X,0:n_Y] -10**-3)\n",
    "        W[T-1,:,:] = Update_W(D[T-1,:,:], beta)\n",
    "        \n",
    "        ############ Calculate subgradients and objective value ################################        \n",
    "        if (W == W_prev).all():\n",
    "            break\n",
    "            \n",
    "    switches = 0\n",
    "    for t in range(0,T-1):\n",
    "        switches += np.absolute(W[t,0:n_X,0:n_Y] - W[t+1,0:n_X,0:n_Y]).sum()\n",
    "\n",
    "    objective = (D * W).sum() + switching_penalty * switches\n",
    "        \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e400f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Subgrad_Opt(X, Y, p, c, mu, max_iter):\n",
    "    \n",
    "    ############ Get input and def constants ###################################################\n",
    "    D = Create_D(X,Y,p,c)\n",
    "    switching_penalty = (mu ** p)/2\n",
    "    n_X = len(X[0,0])\n",
    "    n_Y = len(Y[0,0])\n",
    "    T = len(X[0])\n",
    "\n",
    "    ############ Initilization #################################################################\n",
    "    W = np.zeros((T, n_X+1, n_Y+1))\n",
    "    dual_var = 0.5*switching_penalty*np.ones((T-1, n_X, n_Y))\n",
    "    delta_W = np.zeros((T-1, n_X, n_Y))\n",
    "    MDS = 0.5*np.ones((T-1, n_X, n_Y))\n",
    "    \n",
    "    ############### CALCULATE UPPER BOUND ######################################################\n",
    "    upper_bound = Get_upper_bound(D, switching_penalty)\n",
    "    lower_bound = -np.inf\n",
    "    \n",
    "    last_improvement = 0\n",
    "    alpha = 0.7\n",
    "    lagrangean_obj = lower_bound\n",
    "    \n",
    "    for iterations in range(max_iter):\n",
    "        \n",
    "        delta_lambda = 2*dual_var - switching_penalty\n",
    "        ############ update W ##################################################################\n",
    "        beta = delta_lambda[0,:,:]\n",
    "        W[0,:,:] = Update_W(D[0,:,:], beta)\n",
    "        \n",
    "        for t in range(1,T-1):\n",
    "            beta = -delta_lambda[t-1,:,:] + delta_lambda[t,:,:]\n",
    "            W[t,:,:] = Update_W(D[t,:,:], beta)\n",
    "           \n",
    "        beta = - delta_lambda[T-2,:,:]\n",
    "        W[T-1,:,:] = Update_W(D[T-1,:,:], beta)\n",
    "        \n",
    "        for t in range(1,T):\n",
    "            delta_W[t-1,:,:] = W[t-1,0:n_X,0:n_Y] - W[t,0:n_X,0:n_Y]\n",
    "        \n",
    "        ############ Calculate subgradients and objective value ################################\n",
    "        subgradient = np.copy(delta_W)\n",
    "        lagrangean_obj_prev = lagrangean_obj\n",
    "        lagrangean_obj = (D * W).sum() + (delta_lambda*subgradient).sum()\n",
    "         \n",
    "        ############ Check special cases for projection ########################################   \n",
    "        if (dual_var == 0).any() or (dual_var == switching_penalty).any():\n",
    "            ind1 = np.argwhere(dual_var == 0)\n",
    "            ind2 = np.argwhere(dual_var == switching_penalty)\n",
    "\n",
    "            for i in ind1:\n",
    "                if subgradient[i[0], i[1], i[2]] < 0:\n",
    "                    subgradient[i[0], i[1], i[2]] = 0\n",
    "                if MDS[i[0], i[1], i[2]] < 0:\n",
    "                    MDS[i[0], i[1], i[2]] = 0\n",
    "                    \n",
    "            for i in ind2:\n",
    "                if subgradient[i[0], i[1], i[2]] > 0:\n",
    "                    subgradient[i[0], i[1], i[2]] = 0  \n",
    "                if MDS[i[0], i[1], i[2]] > 0:\n",
    "                    MDS[i[0], i[1], i[2]] = 0  \n",
    "        \n",
    "        if (subgradient==0).all():\n",
    "            lower_bound = lagrangean_obj\n",
    "            break\n",
    "        \n",
    "        if (MDS==0).all():\n",
    "            if lagrangean_obj > lower_bound:\n",
    "                lower_bound = lagrangean_obj \n",
    "            break\n",
    "            \n",
    "        ############ Modified Deflected Subgradient #############################################\n",
    "        theta = max(0, -(subgradient*MDS).sum() / (np.linalg.norm(subgradient)*np.linalg.norm(MDS)))\n",
    "        sigma = 1/(2-theta)\n",
    "        \n",
    "        gamma_ADS = np.linalg.norm(subgradient)/np.linalg.norm(MDS)\n",
    "        gamma_MGT = max(0, -sigma * (subgradient*MDS).sum() / (MDS**2).sum())\n",
    "        gamma_MDS = (1-theta) * gamma_MGT + theta * gamma_ADS\n",
    "        MDS = subgradient + gamma_MDS * MDS\n",
    "\n",
    "        step_size = alpha*(upper_bound - lagrangean_obj)/((MDS**2).sum())\n",
    "        \n",
    "        dual_var = dual_var + step_size * MDS\n",
    "        dual_var[dual_var<0] = 0\n",
    "        dual_var[dual_var>switching_penalty] = switching_penalty\n",
    "            \n",
    "        ############### Terminate and update Alpha ####################### \n",
    "        if lagrangean_obj > lower_bound:\n",
    "            prev_lb = lower_bound\n",
    "            lower_bound = lagrangean_obj \n",
    "            if lower_bound - prev_lb < prev_lb*(10**-6):\n",
    "                small += 1\n",
    "            else:\n",
    "                small = 0\n",
    "                \n",
    "            last_improvement = 0\n",
    "            \n",
    "        if ((upper_bound - lower_bound) < 0.5) or (alpha < 10**-3) or (small == 3): \n",
    "            break\n",
    "            \n",
    "        if last_improvement >= 20:\n",
    "            alpha *= 0.5\n",
    "            last_improvement = 0\n",
    "\n",
    "        last_improvement += 1\n",
    "    \n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0590b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initilization(D, switching_penalty):\n",
    "    \n",
    "    ############ Get input and def constants ###################################################\n",
    "    n_X = len(D[0]) - 1\n",
    "    n_Y = len(D[0,0]) - 1\n",
    "    T = len(D)\n",
    "    \n",
    "    ############ Initilization #################################################################\n",
    "    W = np.zeros((T, n_X+1, n_Y+1))\n",
    "    H = np.zeros((T-1, n_X, n_Y))\n",
    "    S_1 = np.zeros((T-1, n_X, n_Y))\n",
    "    S_2 = np.zeros((T-1, n_X, n_Y))\n",
    "    \n",
    "    for iterations in range(T):\n",
    "    \n",
    "        ############ update W ##################################################################\n",
    "        W_prev = np.copy(W)\n",
    "\n",
    "        beta = switching_penalty * (np.ones((n_X, n_Y)) - W_prev[1,0:n_X,0:n_Y] - 10**-3)\n",
    "        W[0,:,:] = Update_W(D[0,:,:], beta)\n",
    "    \n",
    "        for t in range(1,T-1):\n",
    "            beta = switching_penalty * (np.ones((n_X, n_Y)) - W_prev[t-1,0:n_X,0:n_Y] - W_prev[t+1,0:n_X,0:n_Y] - 10**-3)\n",
    "            W[t,:,:] = Update_W(D[t,:,:], beta)\n",
    "\n",
    "        beta = switching_penalty * (np.ones((n_X, n_Y)) - W_prev[T-2,0:n_X,0:n_Y] - 10**-3)\n",
    "        W[T-1,:,:] = Update_W(D[T-1,:,:], beta)\n",
    "        \n",
    "        ############ Calculate subgradients and objective value ##################### \n",
    "        if (W == W_prev).all():\n",
    "            break\n",
    "        \n",
    "    for t in range(T-1):\n",
    "        H[t,:,:] = np.absolute( W[t,0:n_X,0:n_Y] - W[t+1,0:n_X,0:n_Y] )\n",
    "        S_1[t,:,:] = H[t,:,:] - W[t,0:n_X,0:n_Y] + W[t+1,0:n_X,0:n_Y]\n",
    "        S_2[t,:,:] = H[t,:,:] - W[t+1,0:n_X,0:n_Y] + W[t,0:n_X,0:n_Y]\n",
    "    \n",
    "    return W, H, S_1, S_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b618abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADMM_ordinary(X, Y, p, c, mu, rho, max_iter):\n",
    "\n",
    "    ### Get input and def constants #######################\n",
    "    D = Create_D(X,Y,p,c)\n",
    "    switching_penalty = (mu ** p)/2\n",
    "    n_X = len(X[0,0])\n",
    "    n_Y = len(Y[0,0])\n",
    "    T = len(X[0])\n",
    "    \n",
    "    tol = 10**-4\n",
    "    primal_tol = tol*math.sqrt(n_X*n_Y*(T-1)*2) \n",
    "    dual_tol = tol*math.sqrt(n_X*n_Y*(T-1)*3) \n",
    "\n",
    "    ### Parameters for varing rho ##########################\n",
    "    rho_incr = 2\n",
    "    rho_decr = 2\n",
    "    rho_tol = 10\n",
    "    \n",
    "    ### Initilization ######################################\n",
    "    W, H, S_1, S_2 = Initilization(D, switching_penalty)\n",
    "    \n",
    "    lambda_1 = np.zeros((T-1, n_X, n_Y))\n",
    "    lambda_2 = np.zeros((T-1, n_X, n_Y))\n",
    "    \n",
    "    r = np.zeros((T-1, 2, n_X, n_Y))\n",
    "    \n",
    "    for iterations in range(max_iter):\n",
    "        \n",
    "        ### update W #######################################\n",
    "        W_prev = np.copy(W)\n",
    "        delta_lambda = lambda_1[0,:,:] - lambda_2[0,:,:]\n",
    "        delta_S = S_1[0,:,:] - S_2[0,:,:]\n",
    "        rho_part = delta_S - 2*W[1,0:n_X,0:n_Y] + np.ones((n_X,n_Y))\n",
    "        beta = delta_lambda + rho*rho_part\n",
    "        W[0,:,:] = Update_W(D[0,:,:], beta)\n",
    "        \n",
    "        for t in range(1,T-1):\n",
    "            W_part = - W[t-1,0:n_X,0:n_Y] - W[t+1,0:n_X,0:n_Y] + np.ones((n_X,n_Y))\n",
    "            rho_part = - delta_S + 2*W_part\n",
    "            beta = - delta_lambda + rho*rho_part\n",
    "            delta_lambda = lambda_1[t,:,:] - lambda_2[t,:,:]\n",
    "            delta_S = S_1[t,:,:] - S_2[t,:,:]\n",
    "            beta = beta + delta_lambda + rho*delta_S\n",
    "            W[t,:,:] = Update_W(D[t,:,:], beta)\n",
    "            \n",
    "        rho_part = - delta_S - 2*W[T-2,0:n_X,0:n_Y] + np.ones((n_X,n_Y))\n",
    "        beta = - delta_lambda + rho*rho_part\n",
    "        W[T-1,:,:] = Update_W(D[T-1,:,:], beta)\n",
    "            \n",
    "        ### update H #######################################\n",
    "        H_prev = np.copy(H)\n",
    "        H = switching_penalty * np.ones((T-1,n_X,n_Y))\n",
    "        H = H - lambda_1 - lambda_2 - rho * S_1 - rho * S_2\n",
    "        H = - H/(2*rho)\n",
    "        H[H<0] = 0\n",
    "        \n",
    "        ### update S_1, S_2, lambda_1 and lambda_2 #########\n",
    "        S_1_prev = np.copy(S_1)\n",
    "        S_2_prev = np.copy(S_2)\n",
    "        for t in range(T-1):\n",
    "            delta_W = W[t,0:n_X,0:n_Y] - W[t+1,0:n_X,0:n_Y]\n",
    "            delta_W_H_1 = delta_W - H[t,:,:]\n",
    "            delta_W_H_2 = - delta_W - H[t,:,:]\n",
    "            \n",
    "            ### update S_1 #################################\n",
    "            min_S_1 = - lambda_1[t,:,:]/rho - delta_W_H_1\n",
    "            min_S_1[min_S_1<0] = 0\n",
    "            S_1[t,:,:] = np.copy(min_S_1)\n",
    "\n",
    "            ### update S_2 #################################\n",
    "            min_S_2 = - lambda_2[t,:,:]/rho - delta_W_H_2\n",
    "            min_S_2[min_S_2<0] = 0\n",
    "            S_2[t,:,:] = np.copy(min_S_2)\n",
    "            \n",
    "            ### update lambda_1 ############################\n",
    "            lambda_1[t,:,:] = lambda_1[t,:,:] + rho*(delta_W_H_1 + S_1[t,:,:])\n",
    "\n",
    "            ### update lambda_2 ############################\n",
    "            lambda_2[t,:,:] = lambda_2[t,:,:] + rho*(delta_W_H_2 + S_2[t,:,:])\n",
    "            \n",
    "            r[t,0,:,:] = delta_W_H_1 + S_1[t,:,:]\n",
    "            r[t,1,:,:] = delta_W_H_2 + S_2[t,:,:]\n",
    "            \n",
    "        primal_residual = np.linalg.norm(r) \n",
    "        z = np.stack((2*W[1:T,0:n_X,0:n_Y], S_1, S_2))\n",
    "        z_prev = np.stack((2*W_prev[1:T,0:n_X,0:n_Y], S_1_prev, S_2_prev))\n",
    "        dual_residual = rho*np.linalg.norm((z-z_prev))\n",
    "\n",
    "        if primal_residual < primal_tol and dual_residual < dual_tol:\n",
    "            if False:\n",
    "                print('Number of iteraions: ', iterations+1)\n",
    "                print('Primal residual: ', primal_residual, 'Primal tolerance: ' , primal_tol)\n",
    "                print('Dual residual: ', dual_residual, 'Dual tolerance: ' , dual_tol)\n",
    "                print('W is binary: ', ((W==0) | (W==1)).all())\n",
    "                # print('r is zero: ', ((r==0)).all())\n",
    "            break\n",
    "            \n",
    "        if primal_residual > rho_tol*dual_residual:\n",
    "            rho = rho_incr*rho\n",
    "        elif dual_residual > rho_tol*primal_residual:\n",
    "            rho = rho/rho_decr\n",
    "        \n",
    "    ### Calculate final value #######################################\n",
    "    objective_value = (D * W).sum() + switching_penalty * H.sum()\n",
    "    \n",
    "    return objective_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab914411",
   "metadata": {},
   "outputs": [],
   "source": [
    "R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
